from __future__ import annotations

import logging
from pathlib import Path
from typing import Iterable, List, Optional, Set

import chromadb
from chromadb.api.models.Collection import Collection

from news_scraper.config import settings
from news_scraper.vector_models import VectorDocument

log = logging.getLogger("news_scraper.vectorstore")


class ChromaVectorStore:
    """
    Minimal persistent Chroma wrapper.
    We store embeddings explicitly (generated by our EmbeddingsClient).
    """

    def __init__(self, persist_dir: Optional[Path] = None, collection_name: str = "news_articles") -> None:
        self.persist_dir = persist_dir or settings.chroma_dir
        self.persist_dir.mkdir(parents=True, exist_ok=True)

        self._client = chromadb.PersistentClient(path=str(self.persist_dir))
        self._collection: Collection = self._client.get_or_create_collection(name=collection_name)

    def existing_ids(self) -> Set[str]:
        """
        Fetch all IDs currently stored. For small prototypes this is fine.
        For larger corpora you'd page or maintain an external index.
        """
        ids: Set[str] = set()
        # Chroma allows get(include=[]). We request ids.
        # Note: For very large collections, implement pagination.
        got = self._collection.get(include=[])
        for _id in got.get("ids", []):
            ids.add(str(_id))
        return ids

    def add_documents(self, docs: List[VectorDocument], embeddings: List[List[float]]) -> None:
        if len(docs) != len(embeddings):
            raise ValueError("docs and embeddings length mismatch")

        self._collection.add(
            ids=[d.id for d in docs],
            documents=[d.text for d in docs],
            metadatas=[d.metadata for d in docs],
            embeddings=embeddings,
        )
        log.info("Added %d documents to Chroma", len(docs))
